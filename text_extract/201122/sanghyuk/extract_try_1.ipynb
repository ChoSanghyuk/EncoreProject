{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "# from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name = './data/extractive_test_v2.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(article):\n",
    "    article = re.sub('”', ' ', article)\n",
    "#     article = re.sub('  ', ' ', article)\n",
    "#     article = re.sub('[-=+,#/\\?:^$.@*※~&%ㆍ!』\\\\‘|\\[\\]\\<\\>`\\'…》]', '', article)\n",
    "    bracket = re.findall(r'\\([^)]*\\)', article )\n",
    "    for i in bracket:\n",
    "        word = i.strip('()')\n",
    "        if word.isupper():\n",
    "            end_index = article.find(i)\n",
    "            word_len = article[end_index:0:-1].find(' ')\n",
    "            start_index = end_index - word_len +1\n",
    "            origin = article[start_index : end_index]\n",
    "            article = article[:end_index+len(i)] + article[end_index+len(i):].replace(word, origin)\n",
    "        else:\n",
    "            if '이하' in word:\n",
    "                word = word[3:]\n",
    "                n_space = word.count(' ')\n",
    "                end_index = article.find(word)-4\n",
    "                range_candidate = article[end_index-30:end_index].split(' ')[::-1]\n",
    "                origin = ' '.join(range_candidate[:n_space+1][::-1])\n",
    "                article = article[:end_index+len(i)] + article[end_index+len(i):].replace(word, origin)\n",
    "        # 괄호는 다 제거\n",
    "        article = article.replace(i,'')\n",
    "    article = ''.join(re.findall('[ 가-힣a-zA-Z0-9]',  article) )\n",
    "    \n",
    "    return article.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "def komoran_tokenize(sent):\n",
    "    words = komoran.pos(sent, join=True)\n",
    "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_vocabulary(sents, tokenize, min_count=2):\n",
    "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
    "    counter = {w:c for w,c in counter.items() if c >= min_count}\n",
    "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
    "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
    "    return idx_to_vocab, vocab_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(x, df=0.85, max_iter=30):\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    # initialize\n",
    "    A = normalize(x, axis=0, norm='l1')\n",
    "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
    "    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
    "    \n",
    "    # iteration\n",
    "    for _ in range(max_iter):\n",
    "        R = df * (A * R) + bias\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_sent_sim(s1, s2):\n",
    "    n1 = len(s1)\n",
    "    n2 = len(s2)\n",
    "    if (n1 <= 1) or (n2 <= 1):\n",
    "        return 0\n",
    "    common = len(set(s1).intersection(set(s2)))\n",
    "    base = math.log(n1) + math.log(n2)\n",
    "    return common / base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sent_sim(s1, s2):\n",
    "    if (not s1) or (not s2):\n",
    "        return 0\n",
    "\n",
    "    s1 = Counter(s1)\n",
    "    s2 = Counter(s2)\n",
    "    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n",
    "    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n",
    "    prod = 0\n",
    "    for k, v in s1.items():\n",
    "        prod += v * s2.get(k, 0)\n",
    "    return prod / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_graph(sents, tokenize, similarity, min_count=2, min_sim=0.1):\n",
    "    _, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "\n",
    "    tokens = [[w for w in tokenize(sent) if w in vocab_to_idx] for sent in sents]\n",
    "    rows, cols, data = [], [], []\n",
    "    n_sents = len(tokens)\n",
    "    for i, tokens_i in enumerate(tokens):\n",
    "        for j, tokens_j in enumerate(tokens):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            sim = similarity(tokens_i, tokens_j)\n",
    "            if sim < min_sim:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(sim)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_keysentence(sents, tokenize, min_count, min_sim, similarity, df=0.85, max_iter=30, topk= 3 ):\n",
    "    g = sent_graph(sents, tokenize,  similarity ,min_count, min_sim )\n",
    "    R = pagerank(g, df, max_iter).reshape(-1)\n",
    "    idxs = R.argsort()[-topk:]\n",
    "    key_index = [ idx for idx in reversed(idxs)]\n",
    "#     keysents = [(idx, R[idx], sents[idx]) for idx in reversed(idxs)]\n",
    "#     summary_3 = '\\n'.join( [sents[idx] for idx in reversed(idxs) ]  )\n",
    "    return key_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1 =  pd.DataFrame( columns = ['id' , 'summary'])\n",
    "submission2 =  pd.DataFrame( columns = ['id' , 'summary'])\n",
    "# submission3 =  pd.DataFrame( columns = ['id' , 'summary'])\n",
    "# submission4 =  pd.DataFrame( columns = ['id' , 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 1\n",
    "with open(input_file_name, 'r', encoding = 'utf-8', newline = '') as input_file:\n",
    "    i = 0\n",
    "    for line in input_file:\n",
    "        line = json.loads(line)\n",
    "        id_num, sents = list(line.values())[1:]\n",
    "        preprocessed = [ preprocess_sentence(sent) for sent in sents ]\n",
    "        key_index = textrank_keysentence(preprocessed , komoran_tokenize , 2 , 0.3 , textrank_sent_sim   )\n",
    "          \n",
    "        key_sentence1 ='\\n'.join([sents[t] for t in key_index])\n",
    "#         key_sentence2 ='\\n'.join([sents[t] for t in sorted(key_index)])\n",
    "#         key_sentence3 ='\\n'.join([preprocessed[t] for t in key_index])\n",
    "#         key_sentence4 ='\\n'.join([preprocessed[t] for t in sorted(key_index)])\n",
    "        \n",
    "        row1 = [id_num , key_sentence1 ]\n",
    "#         row2 = [id_num , key_sentence2 ]\n",
    "#         row3 = [id_num , key_sentence3 ]\n",
    "#         row4 = [id_num , key_sentence4 ]\n",
    "        \n",
    "        submission1.loc[i] = row1\n",
    "#         submission2.loc[i] = row2\n",
    "#         submission3.loc[i] = row3\n",
    "#         submission4.loc[i] = row4\n",
    "        \n",
    "        i +=1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1.to_csv('C:/Users/Playdata/Desktop/dacon_extract_summary/extractive_submission.csv', sep=',', index=False)\n",
    "# submission2.to_csv('C:/Users/Playdata/Desktop/dacon_extract_summary/extractive_submission2.csv', sep=',', index=False)\n",
    "# submission3.to_csv('C:/Users/Playdata/Desktop/dacon_extract_summary/extractive_submission3.csv', sep=',', index=False)\n",
    "# submission4.to_csv('C:/Users/Playdata/Desktop/dacon_extract_summary/extractive_submission4.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 2 ***\n",
    "\n",
    "with open(input_file_name, 'r', encoding = 'utf-8', newline = '') as input_file:\n",
    "    i = 0\n",
    "    for line in input_file:\n",
    "        line = json.loads(line)\n",
    "        id_num, sents = list(line.values())[1:]\n",
    "        preprocessed = [ preprocess_sentence(sent) for sent in sents ]\n",
    "        key_index = textrank_keysentence(preprocessed , komoran_tokenize , 2 , 0.1 , textrank_sent_sim   )\n",
    "        key_sentence1 ='\\n'.join([sents[t] for t in key_index])\n",
    "        row1 = [id_num , key_sentence1 ]\n",
    "        submission1.loc[i] = row1\n",
    "        i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1.to_csv('C:/Users/Playdata/Desktop/dacon_extract_summary/extractive_submission2.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try 3 \n",
    "with open(input_file_name, 'r', encoding = 'utf-8', newline = '') as input_file:\n",
    "    i = 0\n",
    "    for line in input_file:\n",
    "        line = json.loads(line)\n",
    "        id_num, sents = list(line.values())[1:]\n",
    "        preprocessed = [ preprocess_sentence(sent) for sent in sents ]\n",
    "        key_index = textrank_keysentence(preprocessed , komoran_tokenize , 2 , 0.1 , textrank_sent_sim , df= 0.5   )\n",
    "        key_sentence1 ='\\n'.join([sents[t] for t in key_index])      \n",
    "        row1 = [id_num , key_sentence1 ]      \n",
    "        submission1.loc[i] = row1     \n",
    "        i +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try 4\n",
    "with open(input_file_name, 'r', encoding = 'utf-8', newline = '') as input_file:\n",
    "    i = 0\n",
    "    for line in input_file:\n",
    "        line = json.loads(line)\n",
    "        id_num, sents = list(line.values())[1:]\n",
    "        preprocessed = [ preprocess_sentence(sent) for sent in sents ]\n",
    "        key_index = textrank_keysentence(preprocessed , komoran_tokenize , 2 , 0.1 , textrank_sent_sim  )\n",
    "        key_sentence1 ='\\n'.join([sents[t] for t in key_index])      \n",
    "        row1 = [id_num , key_sentence1 ]      \n",
    "        submission1.loc[i] = row1     \n",
    "        i +=1         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1.to_csv('C:/Users/Playdata/Desktop/dacon_extract_summary/extractive_submission2.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 5\n",
    "with open(input_file_name, 'r', encoding = 'utf-8', newline = '') as input_file:\n",
    "    i = 0\n",
    "    for line in input_file:\n",
    "        line = json.loads(line)\n",
    "        id_num, sents = list(line.values())[1:]\n",
    "        preprocessed = [ preprocess_sentence(sent) for sent in sents ]\n",
    "        key_index = textrank_keysentence(preprocessed , komoran_tokenize , 2 , 0.1 , cosine_sent_sim )\n",
    "        key_sentence1 ='\\n'.join([sents[t] for t in key_index])      \n",
    "        row1 = [id_num , key_sentence1 ]      \n",
    "        submission2.loc[i] = row1     \n",
    "        i +=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2.to_csv('C:/Users/Playdata/Desktop/dacon_extract_summary/extractive_submission3.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
